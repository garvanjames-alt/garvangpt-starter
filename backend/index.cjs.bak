#!/usr/bin/env node
// index.cjs — GarvanGPT backend (Express, CommonJS)
// RAG v2 with cosine similarity, per-chunk sources, and robust embeddings loading.
// - Loads embeddings from embeddings.json/jsonl if present
// - Also harvests `embedding` arrays directly from memory.jsonl records
// - Adds `source` to used_details (filename#line)

const fs = require("fs");
const path = require("path");
const express = require("express");
const dotenv = require("dotenv");
dotenv.config();

const PORT = process.env.PORT ? Number(process.env.PORT) : 3001;
const PERSONA_MODEL = process.env.PERSONA_MODEL || "gpt-4o-mini";
const EMBEDDING_MODEL = process.env.EMBEDDING_MODEL || "text-embedding-3-small";

// ---------- OpenAI lazy loader (no top-level await in CJS) ----------
let openaiClientPromise = null;
async function getOpenAI() {
  if (!openaiClientPromise) {
    openaiClientPromise = (async () => {
      const mod = await import("openai");
      const OpenAI = mod.default || mod;
      return new OpenAI({ apiKey: process.env.OPENAI_API_KEY });
    })();
  }
  return openaiClientPromise;
}

// ---------- utils ----------
function readJsonl(filePath) {
  if (!fs.existsSync(filePath)) return [];
  const lines = fs.readFileSync(filePath, "utf8").split("\n").filter(Boolean);
  const out = [];
  for (const line of lines) {
    try { out.push(JSON.parse(line)); } catch { /* ignore bad line */ }
  }
  return out;
}

// Return { map, file } so we can log where vectors came from
function loadEmbeddingsWithPath(baseDir) {
  const map = new Map();
  const candidates = [
    path.join(baseDir, "embeddings.json"),
    path.join(baseDir, "embeddings.jsonl"),
    path.join(baseDir, "data", "embeddings.json"),
    path.join(baseDir, "data", "embeddings.jsonl"),
  ];
  let usedFile = null;

  for (const p of candidates) {
    if (!fs.existsSync(p)) continue;
    try {
      if (p.endsWith(".json")) {
        const raw = JSON.parse(fs.readFileSync(p, "utf8"));
        for (const [id, emb] of Object.entries(raw)) map.set(id, emb);
      } else {
        const lines = fs.readFileSync(p, "utf8").split("\n").filter(Boolean);
        for (const line of lines) {
          try {
            const row = JSON.parse(line);
            if (row && row.id && Array.isArray(row.embedding)) {
              map.set(row.id, row.embedding);
            }
          } catch {}
        }
      }
      usedFile = p;
      break; // first one wins
    } catch {
      // ignore malformed file and keep searching
    }
  }

  return { map, file: usedFile };
}

function cosineSim(a, b) {
  if (!a || !b || a.length !== b.length) return 0;
  let dot = 0, na = 0, nb = 0;
  for (let i = 0; i < a.length; i++) {
    const x = a[i], y = b[i];
    dot += x * y; na += x * x; nb += y * y;
  }
  if (na === 0 || nb === 0) return 0;
  return dot / (Math.sqrt(na) * Math.sqrt(nb));
}

// ---------- load memory + embeddings ----------
const BASEDIR = __dirname;
const MEMORY_PATH = path.join(BASEDIR, "memory.jsonl");

let MEMORY = readJsonl(MEMORY_PATH).map(rec => ({
  ...rec,
  source: rec.source || (rec.id ? `${rec.id}#1` : null)
}));

const { map: EMBEDDINGS, file: EMB_FILE } = loadEmbeddingsWithPath(BASEDIR);

// Harvest vectors from memory.jsonl if present in records (backfill scripts often store them there)
let addedFromMemory = 0;
for (const rec of MEMORY) {
  if (!EMBEDDINGS.has(rec.id) && Array.isArray(rec.embedding) && rec.embedding.length > 0) {
    EMBEDDINGS.set(rec.id, rec.embedding);
    addedFromMemory++;
  }
}

// Logs to prove what happened
if (EMB_FILE) console.log(`[boot] embeddings file: ${EMB_FILE}`);
console.log(`[boot] embeddings loaded: ${EMBEDDINGS.size} (+${addedFromMemory} from memory.jsonl)`);

// ---------- express app ----------
const app = express();
app.use(express.json());
app.use((req, res, next) => {
  res.setHeader("Access-Control-Allow-Origin", "*");
  res.setHeader("Access-Control-Allow-Methods", "GET,POST,DELETE,OPTIONS");
  res.setHeader("Access-Control-Allow-Headers", "Content-Type");
  if (req.method === "OPTIONS") return res.sendStatus(200);
  next();
});

// Health
app.get("/health", (req, res) => {
  res.json({
    ok: true,
    persona_model: PERSONA_MODEL,
    embedding_model: EMBEDDING_MODEL,
    memory_records: MEMORY.length
  });
});

// Memory list (lightweight)
function listMemoryHandler(req, res) {
  const items = MEMORY.map(({ id, source }) => ({ id, source: source ?? null }));
  res.json({ items });
}
app.get("/api/memory", listMemoryHandler);
app.get("/memory", listMemoryHandler);

// Dev-only add/clear (ingest script is primary)
app.post("/api/memory", (req, res) => {
  const { id, text, source } = req.body || {};
  if (!text) return res.status(400).json({ error: "text required" });
  const newId = id || `mem_${Date.now()}`;
  const rec = { id: newId, text: String(text), source: source || `${newId}#1` };
  fs.appendFileSync(MEMORY_PATH, JSON.stringify(rec) + "\n", "utf8");
  MEMORY.push(rec);
  return res.json({ ok: true, id: newId });
});
app.delete("/api/memory", (req, res) => {
  fs.writeFileSync(MEMORY_PATH, "", "utf8");
  MEMORY = [];
  return res.json({ ok: true });
});

// ---------- RAG pipeline ----------
async function embed(text) {
  const openai = await getOpenAI();
  const resp = await openai.embeddings.create({
    model: EMBEDDING_MODEL,
    input: text
  });
  return resp.data[0].embedding;
}

async function generateAnswer({ prompt, context }) {
  const openai = await getOpenAI();
  const system = [
    "You are GarvanGPT, a pharmacist educator.",
    "Answer clearly and concisely; use bullet points if helpful.",
    "If guidance varies by country, say so.",
    "This is general information, not personal medical advice."
  ].join(" ");
  const messages = [
    { role: "system", content: system },
    { role: "user", content: `Context:\n${context}\n\nQuestion: ${prompt}` }
  ];
  const chat = await openai.chat.completions.create({
    model: PERSONA_MODEL,
    messages,
    temperature: 0.2
  });
  return chat.choices?.[0]?.message?.content?.trim() || "";
}

app.post("/api/respond", respondHandler);
app.post("/respond", respondHandler);

async function respondHandler(req, res) {
  const t0 = Date.now();
  try {
    const { prompt, min_score } = req.body || {};
    if (!prompt || typeof prompt !== "string") {
      return res.status(400).json({ error: "prompt required" });
    }
    const gate = typeof min_score === "number" ? min_score : 0.0;

    const qEmb = await embed(prompt);

    // Score all docs with vectors
    const scored = [];
    for (const mem of MEMORY) {
      const emb = EMBEDDINGS.get(mem.id);
      if (!emb) continue;
      const score = cosineSim(qEmb, emb);
      scored.push({ mem, score });
    }
    scored.sort((a, b) => b.score - a.score);
    const matches = scored.filter(s => s.score >= gate);

    console.log(
      `[retriever] scored=${scored.length}, gate=${gate.toFixed(2)}, ` +
      `top3=${scored.slice(0,3).map(x => `${x.mem.id}:${x.score.toFixed(3)}`).join(", ")}`
    );

    const top = matches.slice(0, 6);
    const context = top.map(({ mem }) => `# ${mem.id}\n${mem.text}`).join("\n\n");
    const answer = await generateAnswer({ prompt, context });

    const used_details = top.map(({ mem, score }) => ({
      id: mem.id,
      text: mem.text,
      score,
      source: mem.source || `${mem.id}#1`
    }));
    const used_memories = used_details.map(d => d.id);

    console.log(`[respond] used_memories = ${used_memories.join(", ")} (${Date.now()-t0}ms)`);
    res.json({ answer, used_memories, used_details });
  } catch (err) {
    console.error("respond error:", err?.message || err);
    res.status(500).json({ error: "internal_error", detail: String(err?.message || err) });
  }
}

// ---------- boot ----------
app.listen(PORT, () => {
  console.log("✅ Registered routes:");
  console.log("  GET   /health");
  console.log("  GET   /api/memory   (alias: /memory)");
  console.log("  POST  /api/memory   (alias: /memory)");
  console.log("  DELETE /api/memory  (alias: /memory)");
  console.log("  POST  /api/respond  (alias: /respond)");
  console.log();
  console.log(`GarvanGPT backend running at http://localhost:${PORT}`);
  console.log(`Persona model: ${PERSONA_MODEL}`);
  console.log(`Embedding model: ${EMBEDDING_MODEL}`);
  console.log(`Loaded ${MEMORY.length} record(s) from memory.jsonl`);
});
